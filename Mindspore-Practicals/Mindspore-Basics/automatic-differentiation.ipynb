{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e2f8b0-f260-4b8f-a6e2-fcaf9ad4a7bd",
   "metadata": {},
   "source": [
    "#  Automatic Differentiation\n",
    "\n",
    "Backward propagation is the commonly used algorithm for training neural networks. <br>In this algorithm, parameters (model weights) are adjusted based on a gradient of a loss function for a given parameter. <br>The first-order derivative method of MindSpore is mindspore.ops.GradOperation (get_all=False, get_by_list=False, sens_param=False). <br>When get_all is set to False, the first input derivative is computed. <br>When get_all is set to True, all input derivatives are computed. <br>When get_by_list is set to False, weight derivatives are not computed. <br>When get_by_list is set to True, weight derivatives are computed. sens_param scales the output value of the network to change the final gradient. <br>The following uses the MatMul operator derivative for in-depth analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30271da7-8927-4106-8e9f-4d1599c080d7",
   "metadata": {},
   "source": [
    "## Step 1 Compute the first-order derivative of the input. \n",
    "To compute the input derivative, you need to define a network requiring a derivative. <br>The following uses a network f(x,y)=z∗x∗y formed by the MatMul operator as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f3ca14-2ee1-4d4b-9276-939672f44baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.5099998 2.7       3.6000001]\n",
      " [4.5099998 2.7       3.6000001]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import mindspore.nn as nn \n",
    "import mindspore.ops as ops \n",
    "from mindspore import Tensor \n",
    "from mindspore import ParameterTuple, Parameter \n",
    "from mindspore import dtype as mstype \n",
    "\n",
    "class Net(nn.Cell): \n",
    "    def __init__(self): \n",
    "        super(Net, self).__init__() \n",
    "        self.matmul = ops.MatMul() \n",
    "        self.z = Parameter(Tensor(np.array([1.0], np.float32)), name='z') \n",
    "        \n",
    "    def construct(self, x, y): \n",
    "        x = x * self.z \n",
    "        out = self.matmul(x, y) \n",
    "        return out \n",
    "            \n",
    "class GradNetWrtX(nn.Cell): \n",
    "    def __init__(self, net): \n",
    "        super(GradNetWrtX, self).__init__() \n",
    "        self.net = net \n",
    "        self.grad_op = ops.GradOperation() \n",
    "        \n",
    "    def construct(self, x, y): \n",
    "        gradient_function = self.grad_op(self.net) \n",
    "        return gradient_function(x, y) \n",
    "\n",
    "x = Tensor([[0.8, 0.6, 0.2], [1.8, 1.3, 1.1]], dtype=mstype.float32) \n",
    "y = Tensor([[0.11, 3.3, 1.1], [1.1, 0.2, 1.4], [1.1, 2.2, 0.3]], dtype=mstype.float32) \n",
    "\n",
    "output = GradNetWrtX(Net())(x, y) \n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83f102a-e66b-4c9f-9637-95ea2c4b2c5b",
   "metadata": {},
   "source": [
    "## Step 2 Compute the first-order derivative of the weight. \n",
    "To compute weight derivatives, you need to set get_by_list in ops.GradOperation to True. <br>If computation of certain weight derivatives is not required, set requires_grad to False when you definite the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22dcf755-d448-41a3-bf19-affabbf5c185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tensor(shape=[1], dtype=Float32, value= [ 2.15359993e+01]),)\n"
     ]
    }
   ],
   "source": [
    "class GradNetWrtX(nn.Cell): \n",
    "    def __init__(self, net): \n",
    "        super(GradNetWrtX, self).__init__() \n",
    "        self.net = net \n",
    "        self.params = ParameterTuple(net.trainable_params()) \n",
    "        self.grad_op = ops.GradOperation(get_by_list=True) \n",
    "        \n",
    "    def construct(self, x, y):\n",
    "        gradient_function = self.grad_op(self.net, self.params) \n",
    "        return gradient_function(x, y) \n",
    "\n",
    "output = GradNetWrtX(Net())(x, y) \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3456d-3e65-4ab1-8309-657c51bb27cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
