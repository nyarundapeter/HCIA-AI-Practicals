{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41850075-95a6-4314-8ba2-41d49348175f",
   "metadata": {},
   "source": [
    "# Building the Network\n",
    "\n",
    "MindSpore encapsulates APIs for building network layers in the nn module. \n",
    "Different types of neural network layers are built by calling these APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454f176-d58e-4bc6-ab03-8683dcd4e184",
   "metadata": {},
   "source": [
    "## Step 1 : Build a fully-connected layer. \n",
    "In this step, we build a fully-connected layer using the `mindspore.nn.Dense` class.\n",
    "\n",
    "- **Input Tensor:** A 2x3 matrix (`input_a`), representing two samples with three features each.\n",
    "- **Fully-Connected Layer:** \n",
    "  - **in_channels:** 3 (matching the number of input features)\n",
    "  - **out_channels:** 3 (the number of neurons in the layer)\n",
    "  - **weight_init:** Initialized with the value `1` for simplicity.\n",
    "\n",
    "The fully-connected layer (`Dense`) performs a linear transformation on the input tensor, producing an output tensor. The shape of the output tensor is determined by the number of samples (rows) and the `out_channels` specified in the layer.\n",
    "\n",
    "This layer is a fundamental building block in neural networks, used to learn linear relationships between input features and produce the desired output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7e24c9-399f-41da-9d9d-380dcdea972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "import mindspore as ms \n",
    "import mindspore.nn as nn \n",
    "from mindspore import Tensor \n",
    "import numpy as np \n",
    "\n",
    "# Construct the input tensor. \n",
    "input_a = Tensor(np.array([[1, 1, 1], [2, 2, 2]]), ms.float32) \n",
    "print(input_a) \n",
    "\n",
    "# Construct a fully-connected network. Set both in_channels and out_channels to 3. \n",
    "net = nn.Dense(in_channels=3, out_channels=3, weight_init=1) \n",
    "output = net(input_a) \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a86ec-b0be-42b0-bc31-dfc68b98ca84",
   "metadata": {},
   "source": [
    "## Step 2 : Build a convolutional layer\n",
    "\n",
    "### Convolutional Layer Example\r\n",
    "\r\n",
    "In this example, a 2D convolutional layer is created using MindSpore's `nn.Conv2d` class. The parameters for the convolutional layer are as follows:\r\n",
    "\r\n",
    "- **Input Channels:** 1 (e.g., a grayscale image)\r\n",
    "- **Output Channels:** 6 (number of filters)\r\n",
    "- **Kernel Size:** 5x5\r\n",
    "- **Bias:** Not used (`has_bias=False`)\r\n",
    "- **Weight Initialization:** Normal distribution (`weight_init='normal'`)\r\n",
    "- **Padding Mode:** 'Valid', meaning no padding is added (`pad_mode='valid'`)\r\n",
    "\r\n",
    "An input tensor `input_x` is created with the shape `[1, 1, 32, 32]`, representing a batch size of 1, with a single channel, and a 32x32 spatial dimension (e.g., a 32x32 grayscale image).\r\n",
    "\r\n",
    "The shape of the output from the convolutional layer is then printed.\r\n",
    "\r\n",
    "The output shape will be `[1, 6, 28, 28]`, where:\r\n",
    "- 1 is the batch size,\r\n",
    "- 6 is the number of output channels (filters),\r\n",
    "- 28x28 is the spatial dimension of the output, reduced due to the convolution with the 5x5 kern and no padding.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a637d0b3-dcde-4c07-bea1-6a03850613a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(1, 6, 5, has_bias=False, weight_init='normal', pad_mode='valid')\n",
    "input_x = Tensor(np.ones([1, 1, 32, 32]), ms.float32)\n",
    "\n",
    "print(conv2d(input_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afabefa-6de6-4fe4-ad65-7d7cac134790",
   "metadata": {},
   "source": [
    "## Step 3 : Build a ReLU layer\n",
    "\n",
    "In this step, we apply the Rectified Linear Unit (ReLU) activation function using the `mindspore.nn.ReLU` class.\n",
    "\n",
    "- **ReLU Function:** This activation function outputs the input directly if it is positive; otherwise, it outputs zero. It is commonly used in neural networks to introduce non-linearity.\n",
    "\n",
    "- **Input Tensor:** A 1D tensor (`input_x`) with values `[-1, 2, -3, -1]`, represented as `float16`.\n",
    "\n",
    "- **Output:** After applying the ReLU function, all negative values in the input tensor are replaced with `0`, resulting in `[0, 2, 0, 0]`.\n",
    "\n",
    "The ReLU function helps in addressing the vanishing gradient problem and is widely used in deep learning models due to its simplicity and effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c8eae75-76cb-49b9-bf4d-caa5668e6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "input_x = Tensor(np.array([-1, 2, -3, -1]), ms.float16)\n",
    "\n",
    "output = relu(input_x)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e39b2e-48b7-410a-b808-d8a88ad97405",
   "metadata": {},
   "source": [
    "## Step 4 : Build a Pooling Layer\n",
    "In this step, we use the `mindspore.nn.MaxPool2d` class to apply the max pooling operation on a 2D input tensor.\n",
    "\n",
    "- **Max Pooling:** This operation reduces the spatial dimensions of the input by selecting the maximum value from each region defined by the `kernel_size`. This helps in downsampling the input, reducing the number of parameters, and controlling overfitting.\n",
    "\n",
    "- **Kernel Size:** 2x2, meaning that the pooling operation will consider non-overlapping 2x2 regions from the input tensor.\n",
    "- **Stride:** 2, which dictates the step size for the pooling window, effectively halving the spatial dimensions.\n",
    "\n",
    "- **Input Tensor:** A 4D tensor (`input_x`) with a shape of `[1, 6, 28, 28]`, representing:\n",
    "  - 1 batch size,\n",
    "  - 6 channels (e.g., feature maps),\n",
    "  - 28x28 spatial dimensions.\n",
    "\n",
    "- **Output Shape:** After applying the max pooling operation, the spatial dimensions are reduced by half, resulting in an output shape of `[1, 6, 14, 14]`. The number of channels remains unchanged.\n",
    "\n",
    "Max pooling is commonly used in convolutional neural networks (CNNs) to reduce the spatial dimensions of feature maps while retaining the most important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7619b5ea-cdd2-47d3-8ae9-35de8e179238",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "input_x = Tensor(np.ones([1, 6, 28, 28]), ms.float32)\n",
    "\n",
    "print(max_pool2d(input_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14844488-a116-42f4-95e2-6d2260816147",
   "metadata": {},
   "source": [
    "## Step 5 : Build a Flatten Layer\n",
    "In this step, we use the `mindspore.nn.Flatten` class to flatten a multi-dimensional input tensor into a 2D tensor.\n",
    "\n",
    "- **Flatten Operation:** The flattening operation reshapes the input tensor by collapsing all dimensions except the batch size into a single dimension. This is typically used before feeding the data into fully connected layers.\n",
    "\n",
    "- **Input Tensor:** A 4D tensor (`input_x`) with a shape of `[1, 16, 5, 5]`, representing:\n",
    "  - 1 batch size,\n",
    "  - 16 channels,\n",
    "  - 5x5 spatial dimensions.\n",
    "\n",
    "- **Output Shape:** After flattening, the output tensor has a shape of `[1, 400]`, where 400 is the product of the remaining dimensions (`16 * 5 * 5`).\n",
    "\n",
    "Flattening is essential in transitioning from convolutional layers to fully connected layers in a neural network, as it converts the spatial data into a format suitable for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d19284fb-4734-4c69-a353-e90bbe70ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "input_x = Tensor(np.ones([1, 16, 5, 5]), ms.float32)\n",
    "output = flatten(input_x)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87838f36-59c6-46f8-a89b-970966ab6fee",
   "metadata": {},
   "source": [
    "## Define a model class and view parameters\n",
    "The Cell class of MindSpore is the base class for building all network and the basic unit of a network.\n",
    "When a neural network is required, you need to inherit the Cell class and overwrite the __init__ and construct methods.\n",
    "\n",
    "In this step, we define a neural network model using MindSpore by creating a custom class that inherits from the `nn.Cell` base class. The `Cell` class is the foundation for building all neural networks in MindSpore and serves as the basic unit of a network.\n",
    "\n",
    "### Key Components:\n",
    "- **Inheritance from `nn.Cell`:** The `LeNet5` class inherits from `nn.Cell`, which allows us to leverage MindSpore's framework for building and managing neural networks.\n",
    "- **`__init__` Method:** The constructor method is overridden to define the network's layers and operations, including convolutional layers, fully connected layers, ReLU activation, max pooling, and flattening.\n",
    "- **`construct` Method:** This method is overridden to specify how the input data flows through the network, defining the forward pass of the model.\n",
    "\n",
    "### Network Structure:\n",
    "- **Convolutional Layers (`conv1`, `conv2`):** These layers extract features from the input using filters.\n",
    "- **Max Pooling (`max_pool2d`):** Reduces the spatial dimensions while retaining the most important features.\n",
    "- **Fully Connected Layers (`fc1`, `fc2`, `fc3`):** These layers combine the features and map them to the final output.\n",
    "- **ReLU Activation (`relu`):** Introduces non-linearity to the model.\n",
    "- **Flattening (`flatten`):** Converts the 2D feature maps into a 1D vector for the fully connected layers.\n",
    "\n",
    "### Model Instantiation and Viewing Parameters:\n",
    "After defining the `LeNet5` model, we instantiate it and use the `parameters_and_names` method to view the model's parameters. This method provides insight into the model's internal structure, including the weights and biases of each layer.\n",
    "\n",
    "The `LeNet5` network structure follows the classic design used in image classification tasks, particularly inspired by the LeNet architecture, and is often used as a baseline model for simple datasets like MNIST.\n",
    "\n",
    "### Description:\n",
    "- **Input Image:** A grayscale image with dimensions 1x28x28.\n",
    "- **Conv1:** First convolutional layer with 6 filters of size 5x5, outputting feature maps of size 6x24x24.\n",
    "- **ReLU Activation:** Applies the ReLU function to introduce non-linearity.\n",
    "- **Max Pooling:** Reduces spatial dimensions by applying 2x2 pooling, resulting in feature maps of size 6x12x12.\n",
    "- **Conv2:** Second convolutional layer with 16 filters of size 5x5, outputting feature maps of size 16x8x8.\n",
    "- **ReLU Activation:** Applies the ReLU function.\n",
    "- **Max Pooling:** Further reduces dimensions to 16x4x4.\n",
    "- **Flatten:** Converts the 3D feature maps into a 1D vector of size 256.\n",
    "- **FC1:** Fully connected layer with 120 neurons.\n",
    "- **ReLU Activation:** Applies the ReLU function.\n",
    "- **FC2:** Fully connected layer with 84 neurons.\n",
    "- **ReLU Activation:** Applies the ReLU function.\n",
    "- **FC3:** Final fully connected layer with a number of neurons equal to the number of output classes (`num_class`).\n",
    "\n",
    "Below is a graphical representation of the LeNet5 architecture defined in the code:\n",
    "\n",
    "```plaintext\n",
    "Input (1x28x28)       # Input image with 1 channel (e.g., grayscale) and 28x28 pixels\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| Conv2d (6x5x5)  |  # 6 filters of size 5x5, valid padding\n",
    "| Output: 6x24x24 |\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| ReLU            |  # Activation function\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| MaxPool2d (2x2) |  # Pooling with 2x2 kernel, stride 2\n",
    "| Output: 6x12x12 |\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| Conv2d (16x5x5) |  # 16 filters of size 5x5, valid padding\n",
    "| Output: 16x8x8  |\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| ReLU            |  # Activation function\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| MaxPool2d (2x2) |  # Pooling with 2x2 kernel, stride 2\n",
    "| Output: 16x4x4  |\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| Flatten         |  # Flatten the output to 1D\n",
    "| Output: 16*4*4  |\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| Dense (120)     |  # Fully connected layer with 120 neurons\n",
    "| Output: 120     |\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| ReLU            |  # Activation function\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| Dense (84)      |  # Fully connected layer with 84 neurons\n",
    "| Output: 84      |\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| ReLU            |  # Activation function\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "+-----------------+\n",
    "| Dense (num_class)| # Fully connected layer with `num_class` neurons\n",
    "| Output: num_class|\n",
    "+-----------------+\n",
    "\n",
    "       |\n",
    "       v\n",
    "\n",
    "Output (num_class)  # Final output layer providing class probabilitiesclass (10 for MNIST)\r\n",
    "        |\r\n",
    "        V\r\n",
    "  Output\r\n",
    "s) -> Output: num_class dimensions\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34fcb606-7249-4492-97d9-d0474e4b564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Cell): \n",
    "    \"\"\" Lenet network structure \"\"\" \n",
    "    def __init__(self, num_class=10, num_channel=1): \n",
    "        super(LeNet5, self).__init__() \n",
    "        \n",
    "        # Define the required operations. \n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid') \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid') \n",
    "        self.fc1 = nn.Dense(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Dense(120, 84) \n",
    "        self.fc3 = nn.Dense(84, num_class) \n",
    "        self.relu = nn.ReLU() \n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        self.flatten = nn.Flatten() \n",
    "    def construct(self, x): \n",
    "        # Use the defined operations to build a feedforward network. \n",
    "        x = self.conv1(x) \n",
    "        x = self.relu(x) \n",
    "        x = self.max_pool2d(x) \n",
    "        x = self.conv2(x) \n",
    "        x = self.relu(x) \n",
    "        x = self.max_pool2d(x) \n",
    "        x = self.flatten(x) \n",
    "        x = self.fc1(x) \n",
    "        x = self.relu(x) \n",
    "        x = self.fc2(x) \n",
    "        x = self.relu(x) \n",
    "        x = self.fc3(x) \n",
    "        return x \n",
    "# Instantiate the model and use the parameters_and_names method to view the model parameters. \n",
    "modelle = LeNet5() \n",
    "for m in modelle.parameters_and_names(): \n",
    "    print(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
